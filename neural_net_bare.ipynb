{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple Tensorflow example\n",
    "\n",
    "### This code is intended to serve you as a starting point to build Tensorflow code for your own purposes. The examples we will cover will be applied to the case of classification, but some of the principles and tools will be useful for regression also, as well as for unsupervised learning methods such as autoencoders or GANs.\n",
    "\n",
    "\n",
    "#### Requirements:\n",
    " - numpy\n",
    " - matplotlib\n",
    " - tensorflow â€“if you need info on how to install Tensorflow, please follow the steps on the documentation: https://www.tensorflow.org/install/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First import some useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tensorflow likes to use one-hot encoding for categorical variables (a.k.a. dummies in scikit-learn, for instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int2onehot(t_in, num_class, valid_targets):\n",
    "    '''\n",
    "    Convert target value from integer to one-hot.\n",
    "    \n",
    "     PARAMETERS\n",
    "        t_in           : (N)x(1) np.array of target values (integers)\n",
    "        num_class      : number of classes & size of one-hot vector \n",
    "        valid_targets  : list or np.array of valid target values\n",
    "    \n",
    "      RETURNS\n",
    "        t_out          : (N)x(num_class) np.array of one-hot vectors \n",
    "    '''\n",
    "\n",
    "    \n",
    "    # Construct available target values if not passed\n",
    "    if valid_targets == ():\n",
    "        valid_targets = list(set(t_in))\n",
    "\n",
    "    \n",
    "    # Number of data points \n",
    "    N=np.size(t_in)\n",
    "    \n",
    "    \n",
    "    # Initialize to zero and change value where needed\n",
    "    t_out=np.zeros((N, num_class), dtype=int)\n",
    "    for i in range(0,N):\n",
    "        # For this data point, find class\n",
    "        TrueOrFalse = (valid_targets == t_in[i])\n",
    "                   \n",
    "        # Convert list of True/False into 1/0 and\n",
    "        # save into output array of one-hot vectors.\n",
    "        t_out[i] = 1*TrueOrFalse\n",
    "\n",
    "        \n",
    "    return t_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read data from CSV file into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir='./'\n",
    "datafile='donut_2000.csv'\n",
    "data=np.loadtxt(datadir + datafile, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Rearrange data: \n",
    "- separate features and target values\n",
    "- split into training and test sets\n",
    "- one-hot encoding of targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "ifeat=[0,1,2]                    # Indices of feature values\n",
    "itgt = 3                         # Index of target value\n",
    "n_classes = 2                    # Number of classes\n",
    "Nfeat=len(ifeat)\n",
    "\n",
    "x_all=data[:,ifeat]     # features\n",
    "t_all=data[:,itgt]      # target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split into training and test sets, shuffling data points\n",
    "testpercent = 20\n",
    "Ndata=np.size(t_all)\n",
    "Ntes=Ndata*testpercent/100\n",
    "Ntra=Ndata-Ntes\n",
    "\n",
    "ishuff=np.arange(0,Ndata)\n",
    "np.random.shuffle(ishuff)\n",
    "itra=ishuff.tolist()[0:Ntra]\n",
    "ites=ishuff.tolist()[Ntra:Ntra+Ntes]\n",
    "\n",
    "x_train = x_all[itra,:]\n",
    "t_train = t_all[itra]\n",
    "x_test  = x_all[ites,:]\n",
    "t_test  = t_all[ites]\n",
    "\n",
    "\n",
    "\n",
    "# one-hot enconding of target\n",
    "valid_target_values=[0,1]\n",
    "t_train = int2onehot(t_train.reshape(Ntra), n_classes, valid_target_values)\n",
    "t_test  = int2onehot(t_test.reshape(Ntes), n_classes, valid_target_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's visualize the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = True\n",
    "if plotdata:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    fs = 15   # fontsize\n",
    "        \n",
    "    # Select indices of 0 and 1 classes\n",
    "    i0 = t_train[:,0]==1\n",
    "    i1 = t_train[:,1]==1\n",
    "    \n",
    "    # 3d scatter plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    xs = x_train[:,0]\n",
    "    ys = x_train[:,1]\n",
    "    zs = x_train[:,2]\n",
    "    \n",
    "    \n",
    "    ax.scatter(xs[i0], ys[i0], zs[i0], c='r', s=1, alpha=0.2)#, marker=m)\n",
    "    ax.scatter(xs[i1], ys[i1], zs[i1], c='b', s=1, alpha=0.8)#, marker=m)\n",
    "\n",
    "    ax.set_xlabel('x', fontsize=fs)\n",
    "    ax.set_ylabel('y', fontsize=fs)\n",
    "    ax.set_zlabel('z', fontsize=fs)\n",
    "    plt.show();\n",
    "    \n",
    "    \n",
    "    \n",
    "    # xy plot\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(xs[i0], ys[i0], c='r', s=1, alpha=0.2)\n",
    "    plt.scatter(xs[i1], ys[i1], c='b', s=1, alpha=0.8)\n",
    "    plt.xlabel('x', fontsize=fs)\n",
    "    plt.ylabel('y', fontsize=fs)\n",
    "    plt.show();\n",
    "    \n",
    "    \n",
    "    \n",
    "    # xz plot\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(5,2))\n",
    "    plt.scatter(xs[i0], zs[i0], c='r', s=1, alpha=0.2)\n",
    "    plt.scatter(xs[i1], zs[i1], c='b', s=1, alpha=0.8)\n",
    "    plt.xlabel('x', fontsize=fs)\n",
    "    plt.ylabel('z', fontsize=fs)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It's a donut. Yum! --the notebook data_generator.ipynb contains the code that generated this data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let us start with a super simple example: a network with only one neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Single neuron classifier\n",
    "\n",
    "# Learning parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 200\n",
    "batch_size = 250\n",
    "\n",
    "# Optimization\n",
    "\n",
    "opt_type = 'Adam' # 'GD' # \n",
    "\n",
    "\n",
    "# Printout options\n",
    "display_step = 25\n",
    "\n",
    "\n",
    "train_num_examples = Ntra\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_input = Nfeat       # Number of features\n",
    "n_classes = 2         # total classes \n",
    "\n",
    "\n",
    "# Create tensor placeholders for features and targets\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "# NEURAL NET DEFINITION\n",
    "#\n",
    "# Layers weights and biases are initialized with normally\n",
    "# distributed random values.\n",
    "weights = {\n",
    "    'h': tf.Variable(tf.random_normal([n_input,1])),\n",
    "    'out': tf.Variable(tf.random_normal([1, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b': tf.Variable(tf.random_normal([1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "#\n",
    "# Network topology\n",
    "def perceptron(x, weights, biases):\n",
    "    # Single hidden neuron with sigmoid activation\n",
    "    neuron = tf.add(tf.matmul(x, weights['h']), biases['b'])\n",
    "    neuron = tf.nn.sigmoid(neuron)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(neuron, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "\n",
    "# Python list of model internal parameters\n",
    "params  = tf.trainable_variables()\n",
    "\n",
    "\n",
    "# Construct model\n",
    "pred = perceptron(x, weights, biases)\n",
    "\n",
    "\n",
    "# Define cost function: cross-entropy of the softmax \n",
    "# probability distribution on the output layer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "\n",
    "# Cost function optimization routine\n",
    "if opt_type == 'GD':\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "elif opt_type == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "else:\n",
    "    print(\"ERROR: optimizer type unknown. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Method that evaluates if a prediction is correct.\n",
    "# Take output unit \n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "\n",
    "\n",
    "# METRICS\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "epochs = []\n",
    "costs = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(train_num_examples/batch_size)\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            iL_batch = i*batch_size\n",
    "            iR_batch = (i+1)*batch_size\n",
    "            batch_x = x_train[iL_batch:iR_batch, :]\n",
    "            batch_y = t_train[iL_batch:iR_batch, :]\n",
    "\n",
    "            # Run optimization (backpropagation) and cost (to get loss value)\n",
    "            _, c, = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            # Compute average loss in batch\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "\n",
    "            train_acc = accuracy.eval({x: x_train, y: t_train}) \n",
    "            test_acc = accuracy.eval({x: x_test,   y: t_test})\n",
    "            print(\"Epoch:\", epoch, \"  \", \n",
    "                  \"cost=\", round(avg_cost,4), \"  \", \n",
    "                  \"train acc.=\", train_acc, \"  \",  \n",
    "                  \"test acc.=\", test_acc)\n",
    "            \n",
    "            # Save metrics to evaluate convergence\n",
    "            epochs.append(epoch)\n",
    "            costs.append(avg_cost) \n",
    "            train_accs.append(train_acc)\n",
    "            test_accs.append(test_acc)\n",
    "            \n",
    "            \n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy and bias on test set\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Train accuracy:\", accuracy.eval({x: x_train, y: t_train}))\n",
    "    print(\"Test accuracy :\", accuracy.eval({x: x_test,  y: t_test}))\n",
    "    \n",
    "\n",
    "    \n",
    "# Convergence plots\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6), sharey=False)\n",
    "ax1.plot(epochs, costs)\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('cost')\n",
    "\n",
    "ax2.plot(epochs, train_accs, label='train')\n",
    "ax2.plot(epochs, test_accs, label='test')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Neural net with 1 hidden layer\n",
    "\n",
    "# Learning parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 3000\n",
    "batch_size = 50\n",
    "\n",
    "# Optimization\n",
    "\n",
    "opt_type = 'Adam' # 'GD' # \n",
    "\n",
    "\n",
    "# Printout options\n",
    "display_step = 100\n",
    "\n",
    "\n",
    "train_num_examples = Ntra\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_input = Nfeat       # Number of features\n",
    "n_hidden_1 = 500      # 1st layer number of features\n",
    "n_classes = 2         # total classes \n",
    "\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "\n",
    "# NEURAL NET DEFINITION\n",
    "# Layers weights and biases\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# Network\n",
    "def single_layer_net(x, weights, biases):\n",
    "    # Hidden layer with sigmoid activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "\n",
    "# List of model internal parameters\n",
    "params  = tf.trainable_variables()\n",
    "\n",
    "\n",
    "# Construct model\n",
    "pred = single_layer_net(x, weights, biases)\n",
    "\n",
    "\n",
    "# Define cost function:\n",
    "# cross-entropy of the softmax probability distribution on the output layer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "\n",
    "# Cost function optimization routine\n",
    "if opt_type == 'GD':\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "elif opt_type == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "else:\n",
    "    print(\"ERROR: optimizer type unknown. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "    \n",
    "# Method that evaluates if a prediction is correct\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "\n",
    "\n",
    "# METRICS\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "epochs = []\n",
    "costs = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(train_num_examples/batch_size)\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            iL_batch = i*batch_size\n",
    "            iR_batch = (i+1)*batch_size\n",
    "            batch_x = x_train[iL_batch:iR_batch, :]\n",
    "            batch_y = t_train[iL_batch:iR_batch, :]\n",
    "\n",
    "            # Run optimization (backpropagation) and cost (to get loss value)\n",
    "            _, c, = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            # Compute average loss in batch\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "\n",
    "            train_acc = accuracy.eval({x: x_train, y: t_train}) \n",
    "            test_acc = accuracy.eval({x: x_test,   y: t_test})\n",
    "            print(\"Epoch:\", epoch, \"  \", \n",
    "                  \"cost=\", round(avg_cost,4), \"  \", \n",
    "                  \"train acc.=\", train_acc, \"  \",  \n",
    "                  \"test acc.=\", test_acc)\n",
    "            \n",
    "            # Save metrics to evaluate convergence\n",
    "            epochs.append(epoch)\n",
    "            costs.append(avg_cost) \n",
    "            train_accs.append(train_acc)\n",
    "            test_accs.append(test_acc)\n",
    "            \n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy and bias on test set\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Train accuracy:\", accuracy.eval({x: x_train, y: t_train}))\n",
    "    print(\"Test accuracy :\", accuracy.eval({x: x_test,  y: t_test}))\n",
    "    \n",
    "    \n",
    "# Convergence plots\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6), sharey=False)\n",
    "ax1.plot(epochs, costs)\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('cost')\n",
    "\n",
    "ax2.plot(epochs, train_accs, label='train')\n",
    "ax2.plot(epochs, test_accs, label='test')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# 2-layer network\n",
    "\n",
    "# Learning parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 250\n",
    "batch_size = 100\n",
    "\n",
    "# Optimization\n",
    "\n",
    "opt_type = 'Adam' # 'GD' # \n",
    "\n",
    "\n",
    "# Printout options\n",
    "display_step = 25\n",
    "\n",
    "\n",
    "train_num_examples = Ntra\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_input = Nfeat       # Number of features\n",
    "n_hidden_1 = 5        # 1st layer number of units\n",
    "n_hidden_2 = 5        # 2st layer number of units\n",
    "n_classes  = 2        # total classes \n",
    "\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "\n",
    "# NEURAL NET DEFINITION\n",
    "# Layers weights and biases\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_input, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# Network\n",
    "def two_layer_net(x, weights, biases):\n",
    "    # Hidden layer with sigmoid activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    # Hidden layer with ReLu activation\n",
    "    layer_2 = tf.add(tf.matmul(x, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "\n",
    "# List of model internal parameters\n",
    "params  = tf.trainable_variables()\n",
    "\n",
    "\n",
    "# Construct model\n",
    "pred = two_layer_net(x, weights, biases)\n",
    "\n",
    "\n",
    "# Define cost function:\n",
    "# cross-entropy of the softmax probability distribution on the output layer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "\n",
    "# Cost function optimization routine\n",
    "if opt_type == 'GD':\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "elif opt_type == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "else:\n",
    "    print(\"ERROR: optimizer type unknown. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "    \n",
    "# Method that evaluates if a prediction is correct\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "\n",
    "\n",
    "# METRICS\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "epochs = []\n",
    "costs = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(train_num_examples/batch_size)\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            iL_batch = i*batch_size\n",
    "            iR_batch = (i+1)*batch_size\n",
    "            batch_x = x_train[iL_batch:iR_batch, :]\n",
    "            batch_y = t_train[iL_batch:iR_batch, :]\n",
    "\n",
    "            # Run optimization (backpropagation) and cost (to get loss value)\n",
    "            _, c, = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            # Compute average loss in batch\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "\n",
    "            train_acc = accuracy.eval({x: x_train, y: t_train}) \n",
    "            test_acc = accuracy.eval({x: x_test,   y: t_test})\n",
    "            print(\"Epoch:\", epoch, \"  \", \n",
    "                  \"cost=\", round(avg_cost,4), \"  \", \n",
    "                  \"train acc.=\", train_acc, \"  \",  \n",
    "                  \"test acc.=\", test_acc)\n",
    "            \n",
    "            # Save metrics to evaluate convergence\n",
    "            epochs.append(epoch)\n",
    "            costs.append(avg_cost) \n",
    "            train_accs.append(train_acc)\n",
    "            test_accs.append(test_acc)\n",
    "            \n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy and bias on test set\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Train accuracy:\", accuracy.eval({x: x_train, y: t_train}))\n",
    "    print(\"Test accuracy :\", accuracy.eval({x: x_test,  y: t_test}))\n",
    "    \n",
    "    \n",
    "# Convergence plots\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6), sharey=False)\n",
    "ax1.plot(epochs, costs)\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('cost')\n",
    "\n",
    "ax2.plot(epochs, train_accs, label='train')\n",
    "ax2.plot(epochs, test_accs, label='test')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# 3-layer network\n",
    "\n",
    "# Learning parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 250\n",
    "batch_size = 100\n",
    "\n",
    "# Optimization\n",
    "\n",
    "opt_type = 'Adam' # 'GD' # \n",
    "\n",
    "\n",
    "# Printout options\n",
    "display_step = 25\n",
    "\n",
    "\n",
    "train_num_examples = Ntra\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_input = Nfeat       # Number of features\n",
    "n_hidden_1 = 4        # 1st layer number of units\n",
    "n_hidden_2 = 7        # 2st layer number of units\n",
    "n_hidden_3 = 3        # 3rd layer number of units\n",
    "n_classes = 2         # total classes \n",
    "\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "\n",
    "# NEURAL NET DEFINITION\n",
    "\n",
    "# Layers weights and biases\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Net\n",
    "def three_layer_net(x, weights, biases):\n",
    "    # Hidden layer with sigmoid activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    # Hidden layer with ReLU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Hidden layer with sigmoid activation\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.sigmoid(layer_3)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of model internal parameters\n",
    "params  = tf.trainable_variables()\n",
    "\n",
    "\n",
    "# Construct model\n",
    "pred = three_layer_net(x, weights, biases)\n",
    "\n",
    "\n",
    "# Define cost function:\n",
    "# cross-entropy of the softmax probability distribution on the output layer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "\n",
    "# Cost function optimization routine\n",
    "if opt_type == 'GD':\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "elif opt_type == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "else:\n",
    "    print(\"ERROR: optimizer type unknown. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "    \n",
    "# Method that evaluates if a prediction is correct\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "\n",
    "\n",
    "# METRICS\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "epochs = []\n",
    "costs = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(train_num_examples/batch_size)\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            iL_batch = i*batch_size\n",
    "            iR_batch = (i+1)*batch_size\n",
    "            batch_x = x_train[iL_batch:iR_batch, :]\n",
    "            batch_y = t_train[iL_batch:iR_batch, :]\n",
    "\n",
    "            # Run optimization (backpropagation) and cost (to get loss value)\n",
    "            _, c, = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            # Compute average loss in batch\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "\n",
    "            train_acc = accuracy.eval({x: x_train, y: t_train}) \n",
    "            test_acc = accuracy.eval({x: x_test,   y: t_test})\n",
    "            print(\"Epoch:\", epoch, \"  \", \n",
    "                  \"cost=\", round(avg_cost,4), \"  \", \n",
    "                  \"train acc.=\", train_acc, \"  \",  \n",
    "                  \"test acc.=\", test_acc)\n",
    "            \n",
    "            # Save metrics to evaluate convergence\n",
    "            epochs.append(epoch)\n",
    "            costs.append(avg_cost) \n",
    "            train_accs.append(train_acc)\n",
    "            test_accs.append(test_acc)\n",
    "            \n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy and bias on test set\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Train accuracy:\", accuracy.eval({x: x_train, y: t_train}))\n",
    "    print(\"Test accuracy :\", accuracy.eval({x: x_test,  y: t_test}))\n",
    "    \n",
    "    \n",
    "\n",
    "# Convergence plots\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6), sharey=False)\n",
    "ax1.plot(epochs, costs)\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('cost')\n",
    "\n",
    "ax2.plot(epochs, train_accs, label='train')\n",
    "ax2.plot(epochs, test_accs, label='test')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Remember that the 1-layer network was overfitting? L2 regularization is one way to tackle this issue. Let's see how to implement it. Pay close attention to the part of the code were the cost function is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# 1-layer network with L2 regularization\n",
    "\n",
    "# Learning parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 2000\n",
    "batch_size = 50\n",
    "\n",
    "# Optimization\n",
    "opt_type = 'Adam' # 'GD' # \n",
    "\n",
    "# Regularization parameter\n",
    "beta = 0.01\n",
    "\n",
    "# Printout options\n",
    "display_step = 100\n",
    "\n",
    "\n",
    "\n",
    "train_num_examples = Ntra\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_input = Nfeat       # Number of features\n",
    "n_hidden_1 = 500      # 1st layer number of units\n",
    "n_classes = 2         # total classes \n",
    "\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "\n",
    "# NEURAL NET DEFINITION\n",
    "\n",
    "# Layers weights and biases\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Net\n",
    "def one_layer_net(x, weights, biases):\n",
    "    # Hidden layer with sigmoid activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "\n",
    "# List of model internal parameters\n",
    "params  = tf.trainable_variables()\n",
    "\n",
    "\n",
    "# Construct model\n",
    "pred = one_layer_net(x, weights, biases)\n",
    "\n",
    "\n",
    "# Define cost function\n",
    "#\n",
    "# cross-entropy of the softmax probability distribution on the output layer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "# The L2 regularizer is the sum of the squares of all the weights.\n",
    "# Since we have several weight vectors, one for each layer, we\n",
    "# need to sum the L2 terms of each of them\n",
    "regularizer = sum([tf.nn.l2_loss(weights[key]) for key in weights.keys()])\n",
    "\n",
    "# Now, add the regularizer to the cost function\n",
    "cost = cost + beta*regularizer\n",
    "\n",
    "\n",
    "# Cost function optimization routine\n",
    "if opt_type == 'GD':\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "elif opt_type == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "else:\n",
    "    print(\"ERROR: optimizer type unknown. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "    \n",
    "# Method that evaluates if a prediction is correct\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "\n",
    "\n",
    "# METRICS\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "epochs = []\n",
    "costs = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(train_num_examples/batch_size)\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            iL_batch = i*batch_size\n",
    "            iR_batch = (i+1)*batch_size\n",
    "            batch_x = x_train[iL_batch:iR_batch, :]\n",
    "            batch_y = t_train[iL_batch:iR_batch, :]\n",
    "\n",
    "            # Run optimization (backpropagation) and cost (to get loss value)\n",
    "            _, c, = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            # Compute average loss in batch\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "\n",
    "            train_acc = accuracy.eval({x: x_train, y: t_train}) \n",
    "            test_acc = accuracy.eval({x: x_test,   y: t_test})\n",
    "            l2reg = regularizer.eval()\n",
    "            print(\"Epoch:\", epoch, \"  \", \n",
    "                  \"cost=\", round(avg_cost,4), \"  \", \n",
    "                  \"L2=\", l2reg, \"  \", \n",
    "                  \"train acc.=\", train_acc, \"  \",  \n",
    "                  \"test acc.=\", test_acc)\n",
    "            \n",
    "            # Save metrics to evaluate convergence\n",
    "            epochs.append(epoch)\n",
    "            costs.append(avg_cost) \n",
    "            train_accs.append(train_acc)\n",
    "            test_accs.append(test_acc)\n",
    "            \n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy and bias on test set\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Train accuracy:\", accuracy.eval({x: x_train, y: t_train}))\n",
    "    print(\"Test accuracy :\", accuracy.eval({x: x_test,  y: t_test}))\n",
    "    \n",
    "    \n",
    "# Convergence plots\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6), sharey=False)\n",
    "ax1.plot(epochs, costs)\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('cost')\n",
    "\n",
    "ax2.plot(epochs, train_accs, label='train')\n",
    "ax2.plot(epochs, test_accs, label='test')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also wrap our model evaluation into a function.\n",
    "# Notice that the data arrays and other variables such\n",
    "# as the number of features are already in the namespace,\n",
    "# so we do not need to take them as arguments in the wrapper\n",
    "# function â€“although it would be better coding practice.\n",
    "\n",
    "def run_model(learning_rate=0.001,\n",
    "              training_epochs=100,\n",
    "              batch_size=50,\n",
    "              opt_type='GD',\n",
    "              beta=0.01,\n",
    "              display_step=10,\n",
    "              print_steps=False):\n",
    "    '''\n",
    "    Wrapper function for model evaluation\n",
    "    '''\n",
    "    \n",
    "    # Initialize variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "    epochs = []\n",
    "    costs = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.0\n",
    "            total_batch = int(train_num_examples/batch_size)\n",
    "        \n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                iL_batch = i*batch_size\n",
    "                iR_batch = (i+1)*batch_size\n",
    "                batch_x = x_train[iL_batch:iR_batch, :]\n",
    "                batch_y = t_train[iL_batch:iR_batch, :]\n",
    "\n",
    "                # Run optimization (backpropagation) and cost (to get loss value)\n",
    "                _, c, = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "                # Compute average loss in batch\n",
    "                avg_cost += c / total_batch\n",
    "            \n",
    "            \n",
    "            # Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "\n",
    "                train_acc = accuracy.eval({x: x_train, y: t_train}) \n",
    "                test_acc = accuracy.eval({x: x_test,   y: t_test})\n",
    "                l2reg = regularizer.eval()\n",
    "                if print_steps:\n",
    "                    print(\"Epoch:\", epoch, \"  \", \n",
    "                          \"cost=\", round(avg_cost,4), \"  \", \n",
    "                          \"L2=\", l2reg, \"  \", \n",
    "                          \"train acc.=\", train_acc, \"  \",  \n",
    "                          \"test acc.=\", test_acc)\n",
    "            \n",
    "                # Save metrics to evaluate convergence\n",
    "                epochs.append(epoch)\n",
    "                costs.append(avg_cost) \n",
    "                train_accs.append(train_acc)\n",
    "                test_accs.append(test_acc)\n",
    "            \n",
    "    \n",
    "        print(\"Optimization Finished!\")\n",
    "    \n",
    "    \n",
    "        # Calculate accuracy and bias on test set\n",
    "        print(\"Train accuracy:\", accuracy.eval({x: x_train, y: t_train}))\n",
    "        print(\"Test accuracy :\", accuracy.eval({x: x_test,  y: t_test}))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convergence plots\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6), sharey=False)\n",
    "    ax1.plot(epochs, costs)\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.set_ylabel('cost')\n",
    "\n",
    "    ax2.plot(epochs, train_accs, label='train')\n",
    "    ax2.plot(epochs, test_accs, label='test')\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Return final train and test accuracies\n",
    "    return train_accs[-1], test_accs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with this wrapper function we can easily script \n",
    "# our model evaluation and evaluate the performance\n",
    "# as parameters change\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 500\n",
    "batch_size = 10\n",
    "opt_type = 'Adam' # 'GD' # \n",
    "betas = [0.0, 0.001, 0.01, 0.1]\n",
    "display_step = 50\n",
    "\n",
    "train_accs=[]\n",
    "test_accs =[]\n",
    "for beta in betas:\n",
    "    print('beta = ', beta)\n",
    "    tr_a, te_a = run_model(learning_rate, \n",
    "                           training_epochs, \n",
    "                           batch_size, \n",
    "                           opt_type, \n",
    "                           beta, \n",
    "                           display_step)\n",
    "    train_accs.append(tr_a)\n",
    "    test_accs.append(te_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and test accuracies for all betas\n",
    "plt.plot(range(len(train_accs)), train_accs, marker='x', label='train')\n",
    "plt.plot(range(len(test_accs)), test_accs, marker='x', label='test')\n",
    "plt.xlabel('betas', fontsize=15)\n",
    "plt.ylabel('accuracies', fontsize=15)\n",
    "plt.xticks(range(len(train_accs)), betas, rotation='vertical')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing notes\n",
    "#### We have seen a few examples of neural networks for classification. They were not intended to be a thorough machine learning exercise, but rather a gentle intro to how to use Tensorflow. Hopefully you can use some of the pieces of code that you've seen here to build your own neural nets to perform a rigorous ML analysis. \n",
    "\n",
    "#### Have fun! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
